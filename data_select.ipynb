{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from vocab import Vocab\n",
    "\n",
    "def read_languages(data_type, use_saved_vocab): ## train / dev te\n",
    "\n",
    "    chunk_size=1024*1024 # 1MB 단위로 처리\n",
    "    # path = \"/hdd/user15/RNN/dataset/\"\n",
    "    # if data_type == \"train\":\n",
    "    #     en_path = path + f\"processed_train/train_en.txt\"\n",
    "    #     de_path = path + f\"processed_train/train_de.txt\"\n",
    "    # elif data_type == \"dev\":\n",
    "    #     en_path = path + f\"processed_dev/newstest2013.en\"\n",
    "    #     de_path = path + f\"processed_dev/newstest2013.de\"\n",
    "    # elif data_type == \"test\":\n",
    "    #     en_path = path + f\"processed_test/newstest2014-deen-src.en.2\"\n",
    "    #     de_path = path + f\"processed_test/newstest2014-deen-ref.de.2\"\n",
    "\n",
    "    path = \"/home/user15/RNN/dataset5/\"\n",
    "    if data_type == \"train\":\n",
    "        en_path = path + f\"train.en\"\n",
    "        de_path = path + f\"train.de\"\n",
    "    elif data_type == \"dev\":\n",
    "        en_path = path + f\"newstest_2013.en\"\n",
    "        de_path = path + f\"newstest_2013.de\"\n",
    "    elif data_type == \"test\":\n",
    "        en_path = path + f\"newstest2014.en\"\n",
    "        de_path = path + f\"newstest2014.de\"\n",
    "\n",
    "    # Read and parse the text file\n",
    "    with open(en_path, 'r', encoding = 'utf-8') as f1, open(de_path,'r', encoding = 'utf-8') as f2:\n",
    "        #pairs = [[normalize_string(en_line.strip()), normalize_string(de_line.strip())] for en_line, de_line in zip(f1, f2)]\n",
    "        pairs = [[en_line.strip(), de_line.strip()] for en_line, de_line in zip(f1, f2)]\n",
    "\n",
    "        if use_saved_vocab == True:\n",
    "            with open('vocab_if/input_feed_input_vocab.pkl', 'rb') as f1, open('vocab_if/input_feed_output_vocab.pkl', 'rb') as f2:\n",
    "                input_vocab = pickle.load(f1)\n",
    "                output_vocab = pickle.load(f2)\n",
    "        else:\n",
    "            input_vocab = Vocab('en')\n",
    "            output_vocab = Vocab('de')\n",
    "            \n",
    "    return input_vocab, output_vocab, pairs\n",
    "    \n",
    "    \n",
    "    \n",
    "# def filter_pair(pair,max_len):\n",
    "#     is_good_pair = (len(pair[0].split(' ')) <= max_len) and (len(pair[1].split(' ')) <= max_len)\n",
    "#     return is_good_pair\n",
    "\n",
    "def filter_pair(pair,max_len,min_len):\n",
    "    is_good_pair = (len(pair[0].split(' ')) <= max_len) and (len(pair[1].split(' ')) <= max_len)\n",
    "    is_good_pair2 = (len(pair[0].split(' ')) >= min_len) and (len(pair[1].split(' ')) >= min_len)\n",
    "\n",
    "    return is_good_pair & is_good_pair2\n",
    "\n",
    "def filter_pairs(pairs,max_len,min_len):\n",
    "    tf_list = [filter_pair(pair,max_len,min_len) for pair in pairs]\n",
    "    return [pair for pair in pairs if filter_pair(pair,max_len,min_len)], tf_list\n",
    "\n",
    "def prepare_data(data_type, max_len,min_len, use_saved_vocab): ## train / dev / test\n",
    "    input_vocab, output_vocab, pairs = read_languages(data_type, use_saved_vocab)\n",
    "    pairs, tf_list = filter_pairs(pairs,max_len,min_len)\n",
    "\n",
    "\n",
    "    if use_saved_vocab == True:\n",
    "        pass\n",
    "    else:\n",
    "        for pair in pairs:\n",
    "            input_vocab.index_words(pair[0])\n",
    "            output_vocab.index_words(pair[1])\n",
    "            \n",
    "        if input_vocab.n_words > 50000:\n",
    "            input_vocab.trim_vocab(50000)\n",
    "        if output_vocab.n_words > 50000:\n",
    "            output_vocab.trim_vocab(50000)\n",
    "        # 저장\n",
    "        with open('vocab_if/input_feed_input_vocab.pkl', 'wb') as f1, open('vocab_if/input_feed_output_vocab.pkl', 'wb') as f2:\n",
    "            pickle.dump(input_vocab, f1)  # to_dict 메서드를 사용하여 dict로 변환\n",
    "            pickle.dump(output_vocab, f2)  # to_dict 메서드를 사용하여 dict로 변환\n",
    "            \n",
    "    return input_vocab, output_vocab, pairs, tf_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (203575018.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    np.exp(1.7.5)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.exp(1.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab, output_vocab, pairs, tf_list = prepare_data(\"test\",9,7,True)\n",
    "filter_pair_path = \"filter_ref.txt\"\n",
    "\n",
    "with open (filter_pair_path, 'w', encoding= \"utf-8\") as f:\n",
    "\n",
    "     for pair,tf in zip(pairs,tf_list):\n",
    "        f.write(pair[1]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2661, 51])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model import Seq2Seq\n",
    "from dataset2 import NMT_Dataset, prepare_data, tensor_from_pair\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "## Device ##\n",
    "args.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Eval ##\n",
    "args.criterion = torch.nn.CrossEntropyLoss(ignore_index=2)  # Optional ignore padding\n",
    "\n",
    "\n",
    "\n",
    "## Data sttting parameter ##\n",
    "args.is_shuffle = True\n",
    "\n",
    "## Gradient/Optimizer ##\n",
    "args.max_grad_norm = 5\n",
    "args.learning_rate_gamma = 0.5\n",
    "\n",
    "## Path ##\n",
    "args.best_loss_model_path = 'model_new/best_loss_model_2.pth'\n",
    "\n",
    "\n",
    "## Model ##\n",
    "args.hidden_dimension = 1000\n",
    "args.max_len = 10\n",
    "args.min_len =0\n",
    "args.n_layers = 4\n",
    "args.dropout = 0.0\n",
    "\n",
    "## Learning ##\n",
    "args.lr = 1.0\n",
    "args.epoch = 10\n",
    "args.batch_size = 128\n",
    "args.lr_milestone = [6, 7, 8, 9, 10]\n",
    "args.use_saved_vocab = False\n",
    "\n",
    "## Options depend on dropout version\n",
    "if args.dropout > 0.0:\n",
    "    args.epoch = 12\n",
    "    args.lr_milestone = [8, 9, 10, 11, 12]\n",
    "\n",
    "## data prepareation:\n",
    "\n",
    "\n",
    "input_vocab, output_vocab, test_pairs = prepare_data('test',args.max_len, args.min_len,True)\n",
    "\n",
    "\n",
    "test_input_tensors, test_output_tensors,test_source_len_tensors = tensor_from_pair(test_pairs, input_vocab, output_vocab,args.max_len, \"test\")\n",
    "del test_pairs\n",
    "\n",
    "\n",
    "test_nmt_dataset = NMT_Dataset(test_input_tensors, test_output_tensors)\n",
    "test_dataloader = DataLoader(test_nmt_dataset, batch_size = args.batch_size, shuffle = False, drop_last=False)\n",
    "\n",
    "test_output_tensors.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
